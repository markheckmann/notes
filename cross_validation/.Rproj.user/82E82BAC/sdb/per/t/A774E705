{
    "contents" : "---\ntitle: \"Cross validation\"\nauthor: \"by Mark Heckmann\"\ndate: \"3. November 2014\"\noutput: \n  html_document:\n    theme: united\n    toc: yes\n    toc_depth: 3\n---\n\n```{r echo=FALSE}\nknitr::opts_chunk$set(comment=NA, cache=TRUE)\n```\n\n```{r}\nlibrary(ISLR)\nlibrary(boot)\n```\n\nBased on: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014\n\n```{r}\nplot(mpg ~ horsepower, data=Auto, pch=16)\n```\n\n# Leave-one-out cross validation (LOOCV)\n\n```{r}\nm = glm(mpg ~ horsepower, data=Auto)\ncv = cv.glm(Auto, glmfit = m)\ncv$delta    # cv prediction error: 1) raw result, \n            # 2) bias corrected as traning set is smaller than original\n```\n\nThe average squared prediction error is\n\n$$\n\\frac{1}{n} \\sum_{i=1}^n{(y_i - \\hat y_{-i})^2}\n$$\n\nwhere $\\hat y_{-i}$ is the prediction based on the leave-one-out training set, i.e. the sample without observation $i$. This aaproac is computer intense as it fits $n$ seperate models.\nLeave one out CV can be done much faster using the following formula, where $H_{ii}$ is the element number $i$ of the diagonal of the hat matrix in linear regression. The hat matrix is the projection matrix that projects the outcome observation vector onto the columns space spanned by the observed variable vectors. \n\n$$\n\\frac{1}{n} \\sum_{i=1}^n{ \\frac{(y_i - \\hat y_i)^2}{ (1 - H_{ii} )^2}  }\n$$\n\nImplementing this approach is much faster than the brute force approach of fitting $n$ models, as it only requires the estimation of *one* model. Note though that it is non bias corrected version.\n\n```{r}\nloocv <- function(fit) {\n  h <- influence(fit)$hat\n  mean(residuals(fit)^2 / (1-h)^2)\n}\nloocv(m)\n```\n\nNow we will get the LOOCV prediction error for different models with polynomials of increasing power.\n\n```{r}\ncv.error = rep(NA, 5)   # init result vector\ndegree = 1:5\nfor (d in degree) {\n  fit <- glm(mpg ~ poly(horsepower, d), data=Auto)  \n  cv.error[d] = loocv(fit)\n}\nplot(degree, cv.error, type=\"b\")\n```\n\n\n# k-fold cross validation\n\nThis approach is less computation intense compared to LOOCV as we fit $k$ (usually 5 or 10), not $n$ models.\n\n```{r}\nplot(degree, cv.error, type=\"b\")\nk = 10\ncv.error = rep(NA, 5)   # init result vector\ndegree = 1:5\nfor (d in degree) {\n  fit <- glm(mpg ~ poly(horsepower, d), data=Auto)  \n  cv.error[d] = cv.glm(Auto, fit, K = k)$delta[1]\n}\nlines(degree, cv.error, type=\"b\", col=\"red\")     # overlay\n```\n\n## Example weather forecast\n\ndata from: http://www.weather.uwaterloo.ca/data.html#archive\n```{r}\n# #read waterloo temperature data\n# files <- dir(\"data\", full.names = TRUE)\n# l <- list()\n# for (i in seq_along(files)) {\n#   x <- read.csv(files[i])\n#   x$index <- 1:nrow(x)\n#   x$year <- i\n#   l[[i]] <- x\n# }\n# x <- do.call(rbind, l)\n# x <- subset(x, index <= 365)  # remove day 366\n\nx <- read.csv(\"data/Daily_summary_2007.csv\")\nx$index <- 1:nrow(x)\nplot(x$index, x$High.temperature, pch=16, cex=.5)\n```\n\nFitting \n\n```{r}\ndegree = 1:12\nmm = list()\nm.error = rep(NA, 10)\nfor (d in degree) {\n  mm[[d]] <- lm(High.temperature ~ poly(index, d), data=x)\n  m.error[d] = mean(residuals(mm[[d]])^2)\n}\nps = c(3,12)\nplot(x$index, x$High.temperature, pch=16, cex=.5, las=1)\nlines(x$index, predict(mm[[ps[1]]]), col=\"red\", lwd=2)\nlines(x$index, predict(mm[[ps[2]]]), col=\"blue\", lwd=2)\nlegend(\"topleft\", legend = paste(c(\"Polynomial of order\"), ps), \n       fill = c(\"red\", \"blue\"), cex = .7)\n```\n\n```{r}\n\nx <- read.csv(\"data/Daily_summary_2007.csv\")\nx$index <- 1:nrow(x)\nx2 <- read.csv(\"data/Daily_summary_2009.csv\")\nx2$index <- 1:nrow(x)\nplot(x$index, x$High.temperature, pch=16, cex=.5, las=1)\npoints(x2$index, x2$High.temperature, pch=16, cex=.5, col=\"red\")\n\nm.pred.error = NA\nm.error = NA\ncv.error = NA\nloocv.error = NA \ndegree = 1:20\nk=5\nfor (d in degree) {\n  m <- glm(High.temperature ~ poly(index, d), data=x)\n  m.error[d] = mean(residuals(m)^2)\n  cv.error[d] = cv.glm(x, m, K = k)$delta[1]\n  loocv.error[d] = loocv(m)\n  m.pred.error[d] =  mean((x2$High.temperature - predict(m, newdata=x2))^2)\n}\n# error by polynomial\nplot(degree, m.error, type=\"l\", col=\"darkgreen\", las=1, xlim=c(2, 20))\npoints(degree, m.error, pch=16, col=\"darkgreen\")\nlines(degree, m.pred.error, col=\"red\")\npoints(degree, m.pred.error, pch=16, col=\"red\")\nlines(degree, cv.error, col=\"blue\")\npoints(degree, cv.error, pch=16, col=\"blue\")\nlines(degree, loocv.error, col=\"brown\")\npoints(degree, loocv.error, pch=16, col=\"brown\")\n\n```\n\n",
    "created" : 1438993384879.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1297961741",
    "id" : "A774E705",
    "lastKnownWriteTime" : 1438993396,
    "path" : "~/Dropbox/_mh/uni/writings/cross_validation/cross_validation.Rmd",
    "project_path" : "cross_validation.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}